\section{Related work}

The current status of work and challenges for self adaptive systems are well summarized in the work of Salehie et. al. \cite{selfAdaptation1}. They also summarize some work done in practice by some of the organizations. Kruptizer et. al. \cite{selfAdaptation2} discusses different engineering approaches used in building adaptive software systems. The approaches they discuss include but not limited to model-based, architecture-based, reflective, programming paradigms, control theory, service oriented, agent based, formal modeling and verification based, machine learning based and others \cite{selfAdaptation2}. Tactical Situational Awareness (TSA) system that we worked with falls somewhere between Model based approach \cite{modelBasedAdaptation} and Architecture based approach \cite{architectureBasedAdaptation} to build self adaptive software systems.  


Alex or Arpit - if space permits, add something mentioning slicing.

Delta-debugging (DD/dd for short) is an algorithm for reducing the size of failing test cases or test inputs \cite{ddOriginal}. Hierarchical Delta Debugging \cite{hddOriginal} or HDD/hdd was proposed to efficiently reduce test inputs that are hierarchical in nature, for example, html inputs, xml inputs and programs. When programs were reduced earlier using HDD, the reduction criterion were simpler like keeping the crash. Many HDD implementations have been proposed previously, including a very recent one priceley \cite{hddOriginal,chipperJ,pricieley}. Early HDD algorithm paper has reducer for C program \cite{hddOriginal}. CReduce was proposed to reduce C Programs generated to find compiler bugs in C program \cite{cReduce}. First of all our tool differs from those tools in its intended usage. Almost all other tools were designed to reduce test inputs to find the smallest failing test input. Our tool try to reduce program with intention to find a reduced but useful version of the program that does not respect certain specifications as specified in test suite. Most of the previous program reducers find minimal configuration using AST (like we do), most of them does not rely on statement deletion as their primary reduction step \cite{hddOriginal,hddOriginal} 

Also, all other tools, because they do not need to respect any property that does not contain failure, normally produce outputs that are significantly smaller then original output. Most of the papers boast their reduction factors and compare with other approaches using reduction factors \cite{hddOriginal,chipperJ,pricieley}. In order to quickly generate those reduced input, they tend to have bigger chops earlier when they reduce. In our case, a program and its reduced version will have few dissimilarity as the reduced program is expected to be a useful artifact itself, our algorithm (1) starts with the smallest unit possible (sentence, in our case) and (2) Start with leaf nodes and build upwards.  

Authors of the most recent tool picinery  \cite{pricieley}, lamented the fact that no newer HDD tools came out for long years and older tools use very outdated technology. Authors also acknowledge the fact that HDD tools are not used in practice. They proposed a new tool, picinery, where they use homogeneous technology component (every underlying component of their tool uses python) and used modern libraries like ECFG ( Extended CFG) instead of CFG(Context Free Grammer). Authors definitely modernize HDD, but did not mention how their tool will encourage widespread industry usage. 

%chipperJ reduces java programs for automated bug isolation and was successfully applied to reduce complex java programs for bug isolation \cite{chipperJ}. chipperJ rely on very naive symptom like null pointer exception or looking for particular string in the output and cannot be used for general purpose standalone tool with modern day software without significant changes.%

Most HDD (and DD) implementation assumes the ability to provide test results into HDD implementation in the format that the tool wants. That means a practitioner will have to make sure that they write some sort of test script and plug in the results in the format the tool expects. picinery has \texttt{test \"put your test script here\"} option in command and chipperJ architecture figure has a test script block supplying results to chipperJ.  Before any HDD reductions will start, a pre processing step is expected and mostly performed when HDD tools are run for research purpose. 

Rahul writes about statement deletion mutatation. 
Rahul writes about unchecked coverage.
 
%With modern day software system, there is an ecosystem of programming and testing technology. Most of the open source and even industry Java projects have their tests written in JUnit. Same is true with other programming languages like C\#, javascript etc. HDD tools fail to leverage this fact leaving some of the work as pre-processing for developers. As we will discuss later, hddRASS is a complete standalone tool that requires no pre-processing if Java/Android program is used with Junit \cite{junitHome} test suite. Also, as we will discuss in tool section, our tool is easily extendable to other test framework in the ecosystem.
%
%Mutation \cite{mutation} was technique to inject fault in the program to measure effectiveness of the test suite. SDL \cite{sdlMutation} is statement deletion mutation. HDD based program reductions are essentially applying SDL mutation at each step of reduction. They just apply it in a very systematic way with the end goal of keeping the fault and building on the last useful mutant generated until a local minimal mutant is found, in our case, end goal is keeping program correct with respect to test suite.%    
 

    

   

